{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination and Large Result Sets\n",
    "\n",
    "This notebook demonstrates how to efficiently work with large result sets using:\n",
    "- The `paginate()` method for automatic pagination\n",
    "- Manual pagination with `offset` and `limit`\n",
    "- Best practices for handling large datasets\n",
    "\n",
    "## When to Use Pagination\n",
    "\n",
    "Use pagination when:\n",
    "- You expect more than 50-100 results\n",
    "- You want to process results in batches\n",
    "- You need to implement infinite scroll or \"load more\"\n",
    "- Memory constraints require processing data in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Add parent directory to path for local development\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('..'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanohubremote import Session\n",
    "from nanohubresults import Results\n",
    "\n",
    "# Initialize session\n",
    "auth_data = {\n",
    "    \"grant_type\": \"personal_token\",\n",
    "    \"token\": \"YOUR_TOKEN_HERE\"\n",
    "}\n",
    "session = Session(auth_data, url=\"https://nanohub.org/api\")\n",
    "results = Results(session)\n",
    "\n",
    "print(\"âœ“ Connected to nanoHUB API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Automatic Pagination with `paginate()`\n",
    "\n",
    "The `paginate()` method automatically fetches results in pages and yields them one at a time. This is the easiest way to iterate over large result sets.\n",
    "\n",
    "### How it works:\n",
    "1. Fetches results in pages (default: 50 per page)\n",
    "2. Automatically handles offset increments\n",
    "3. Stops when no more results are available\n",
    "4. Yields individual results, not pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up pagination query...\")\n",
    "\n",
    "query = results.query(\"2dfets\", simtool=False) \\\n",
    "    .filter(\"input.Ef\", \">\", 0) \\\n",
    "    .select(\"input.Ef\", \"input.Lg\", \"output.f11\")\n",
    "\n",
    "print(\"Query ready. Will fetch results in pages of 10.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Pagination Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iterating over results...\\n\")\n",
    "print(f\"{'#':<6} {'SQUID':<50} {'Ef (V)':<10} {'Lg (nm)':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "count = 0\n",
    "for result in query.paginate(per_page=10):\n",
    "    count += 1\n",
    "    \n",
    "    # Show details for first 5 results\n",
    "    if count <= 5:\n",
    "        squid = result.get('squid', '')[:47] + '...' if len(result.get('squid', '')) > 50 else result.get('squid', '')\n",
    "        ef = result.get('input.Ef', 'N/A')\n",
    "        lg = result.get('input.Lg', 'N/A')\n",
    "        print(f\"{count:<6} {squid:<50} {ef:<10} {lg:<10}\")\n",
    "    elif count == 6:\n",
    "        print(\"... (processing remaining results)\")\n",
    "    \n",
    "    # Safety limit for demo\n",
    "    if count >= 50:\n",
    "        print(f\"\\nReached demo limit of 50 results\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ“ Processed {count} results total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Manual Pagination\n",
    "\n",
    "For more control, you can implement pagination manually using `offset` and `limit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Manual pagination example\\n\")\n",
    "\n",
    "page_size = 10\n",
    "page_num = 0\n",
    "total_results = 0\n",
    "\n",
    "while page_num < 3:  # Fetch first 3 pages\n",
    "    offset = page_num * page_size\n",
    "    \n",
    "    print(f\"Fetching page {page_num + 1} (offset={offset}, limit={page_size})...\")\n",
    "    \n",
    "    page_query = results.query(\"2dfets\", simtool=False) \\\n",
    "        .filter(\"input.Ef\", \">\", 0.2) \\\n",
    "        .select(\"input.Ef\", \"input.Lg\") \\\n",
    "        .limit(page_size) \\\n",
    "        .offset(offset)\n",
    "    \n",
    "    response = page_query.execute()\n",
    "    page_results = response.get('results', [])\n",
    "    \n",
    "    if not page_results:\n",
    "        print(\"  No more results\")\n",
    "        break\n",
    "    \n",
    "    print(f\"  Retrieved {len(page_results)} results\")\n",
    "    total_results += len(page_results)\n",
    "    page_num += 1\n",
    "\n",
    "print(f\"\\nâœ“ Total results fetched: {total_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data in Batches\n",
    "\n",
    "For large datasets, you might want to process and save data in batches to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"Processing results in batches...\\n\")\n",
    "\n",
    "batch_query = results.query(\"2dfets\", simtool=False) \\\n",
    "    .filter(\"input.Ef\", \">\", 0) \\\n",
    "    .select(\"input.Ef\", \"input.Lg\", \"input.temperature\")\n",
    "\n",
    "batch_size = 10\n",
    "batch_num = 0\n",
    "batch_data = []\n",
    "\n",
    "for result in batch_query.paginate(per_page=10):\n",
    "    batch_data.append(result)\n",
    "    \n",
    "    # When batch is full, save it\n",
    "    if len(batch_data) >= batch_size:\n",
    "        batch_file = f\"batch_{batch_num:03d}.json\"\n",
    "        with open(batch_file, 'w') as f:\n",
    "            json.dump(batch_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved batch {batch_num} to {batch_file} ({len(batch_data)} results)\")\n",
    "        \n",
    "        batch_num += 1\n",
    "        batch_data = []\n",
    "    \n",
    "    # Demo limit\n",
    "    if batch_num >= 3:\n",
    "        break\n",
    "\n",
    "# Save any remaining data\n",
    "if batch_data:\n",
    "    batch_file = f\"batch_{batch_num:03d}.json\"\n",
    "    with open(batch_file, 'w') as f:\n",
    "        json.dump(batch_data, f, indent=2)\n",
    "    print(f\"Saved final batch {batch_num} to {batch_file} ({len(batch_data)} results)\")\n",
    "\n",
    "print(f\"\\nâœ“ Saved {batch_num + 1} batch files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Statistics Across Pages\n",
    "\n",
    "You can efficiently collect statistics without loading all data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collecting statistics across all pages...\\n\")\n",
    "\n",
    "stats_query = results.query(\"2dfets\", simtool=False) \\\n",
    "    .filter(\"input.Ef\", \">\", 0) \\\n",
    "    .select(\"input.Ef\", \"input.Lg\", \"input.temperature\")\n",
    "\n",
    "# Initialize statistics\n",
    "stats = {\n",
    "    'count': 0,\n",
    "    'ef_sum': 0,\n",
    "    'ef_min': float('inf'),\n",
    "    'ef_max': float('-inf'),\n",
    "    'lg_sum': 0,\n",
    "    'temp_counts': {}\n",
    "}\n",
    "\n",
    "# Process results\n",
    "for result in stats_query.paginate(per_page=20):\n",
    "    stats['count'] += 1\n",
    "    \n",
    "    ef = result.get('input.Ef', 0)\n",
    "    lg = result.get('input.Lg', 0)\n",
    "    temp = result.get('input.temperature')\n",
    "    \n",
    "    stats['ef_sum'] += ef\n",
    "    stats['ef_min'] = min(stats['ef_min'], ef)\n",
    "    stats['ef_max'] = max(stats['ef_max'], ef)\n",
    "    stats['lg_sum'] += lg\n",
    "    \n",
    "    if temp:\n",
    "        stats['temp_counts'][temp] = stats['temp_counts'].get(temp, 0) + 1\n",
    "    \n",
    "    # Demo limit\n",
    "    if stats['count'] >= 100:\n",
    "        break\n",
    "\n",
    "# Calculate averages\n",
    "if stats['count'] > 0:\n",
    "    print(\"Statistics Summary:\")\n",
    "    print(f\"  Total results: {stats['count']}\")\n",
    "    print(f\"\\n  Fermi Energy:\")\n",
    "    print(f\"    Min: {stats['ef_min']:.3f} V\")\n",
    "    print(f\"    Max: {stats['ef_max']:.3f} V\")\n",
    "    print(f\"    Avg: {stats['ef_sum'] / stats['count']:.3f} V\")\n",
    "    print(f\"\\n  Gate Length:\")\n",
    "    print(f\"    Avg: {stats['lg_sum'] / stats['count']:.1f} nm\")\n",
    "    print(f\"\\n  Temperature distribution:\")\n",
    "    for temp, count in sorted(stats['temp_counts'].items()):\n",
    "        print(f\"    {temp} K: {count} results ({count/stats['count']*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"No results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance of different page sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Performance comparison of different page sizes:\\n\")\n",
    "print(f\"{'Page Size':<12} {'Time (s)':<12} {'Results':<10} {'Pages':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for page_size in [5, 10, 20, 50]:\n",
    "    perf_query = results.query(\"2dfets\", simtool=False) \\\n",
    "        .filter(\"input.Ef\", \">\", 0.25) \\\n",
    "        .filter(\"input.Ef\", \"<\", 0.35) \\\n",
    "        .select(\"input.Ef\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    count = 0\n",
    "    pages = 0\n",
    "    \n",
    "    for result in perf_query.paginate(per_page=page_size):\n",
    "        count += 1\n",
    "        if count % page_size == 1:\n",
    "            pages += 1\n",
    "        \n",
    "        # Limit to 50 results for fair comparison\n",
    "        if count >= 50:\n",
    "            break\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"{page_size:<12} {elapsed:<12.3f} {count:<10} {pages:<10}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Tip: Larger page sizes are generally faster but use more memory per page.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "1. âœ“ How to use `paginate()` for automatic pagination\n",
    "2. âœ“ How to implement manual pagination with offset/limit\n",
    "3. âœ“ How to process large datasets in batches\n",
    "4. âœ“ How to collect statistics efficiently\n",
    "5. âœ“ Performance considerations for different page sizes\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### Choose the Right Page Size\n",
    "- **Small (10-20)**: Good for interactive applications, lower memory\n",
    "- **Medium (50)**: Default, balanced performance\n",
    "- **Large (100+)**: Faster for bulk processing, higher memory\n",
    "\n",
    "### When to Use Each Method\n",
    "- **`paginate()`**: When you want to process all results sequentially\n",
    "- **Manual pagination**: When you need specific pages (e.g., page 5 only)\n",
    "- **Batch processing**: When dealing with very large datasets\n",
    "\n",
    "### Memory Management\n",
    "- Process data as you iterate, don't collect all results first\n",
    "- Save to disk in batches for very large datasets\n",
    "- Use generators and iterators to minimize memory usage\n",
    "\n",
    "### Performance Tips\n",
    "- Select only the fields you need\n",
    "- Use filters to reduce result set size\n",
    "- Consider caching frequently accessed data\n",
    "- Balance page size between API calls and memory usage\n",
    "\n",
    "## Advanced Topics\n",
    "\n",
    "For production applications, consider:\n",
    "- Implementing retry logic for failed requests\n",
    "- Adding progress bars (e.g., with `tqdm`)\n",
    "- Parallel processing of pages\n",
    "- Caching strategies with `requests-cache`\n",
    "- Database storage for large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}